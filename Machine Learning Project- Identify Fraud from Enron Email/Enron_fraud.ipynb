{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5 Enron Fraud Identification  \n",
    "\n",
    "For better communication on logic flow, I create this RMD, and arrange it into 6 parts corresponding to the 6 questions asked.  \n",
    "  \n",
    "  \n",
    "### 1.Project Overview and Preliminary Data Exploration \n",
    ">*Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?*  \n",
    "\n",
    "#### 1.1 Goal of Project\n",
    "[Enron Scandal](https://en.wikipedia.org/wiki/Enron_scandal), revealed in 2001, is one of the most infamous bankrupcy scandal in human history. Some scholars believe it is the signal of forthcoming economic crisis, as it heavily damaged the confidence of common people in stock and capital market. This event leads to the collapse of Anderson, the bigggest accountancy back then, and pushes the release of a series of profound laws and policies in related field.  \n",
    "It involves huge amount of capital and personnel, especially managing class of Enron, who earned astronomical number and greedy for even more. Now we know their crime is not made by pulse, rather they did it systematically for years. There must be  some clues we can use for detective tracking. Our goal is using economic and email features offerred in the dataset,try to draw a skech of criminals. Or more clear, build a binary classifier with supervised(labels) machine learning to differentiate criminal candidates from the innocent (POI and non-POI). \n",
    "\n",
    "#### 1.2 Role of Machine Learning\n",
    "Unlike human beings, Machine Learning can combine huge amount of features and make sense of them by modeling. It's especially useful when the relationship between things is complicated and intangling. By clearly identifying the question and carefully deal with features, we are able to get closer and closer to truth.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>expenses</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>1.250000e+02</td>\n",
       "      <td>1.020000e+02</td>\n",
       "      <td>8.200000e+01</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>1.260000e+02</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>6.600000e+01</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.123288</td>\n",
       "      <td>5.621943e+05</td>\n",
       "      <td>2073.860465</td>\n",
       "      <td>1.642674e+06</td>\n",
       "      <td>5.081526e+06</td>\n",
       "      <td>5.987054e+06</td>\n",
       "      <td>2.374235e+06</td>\n",
       "      <td>2.321741e+06</td>\n",
       "      <td>1176.465116</td>\n",
       "      <td>1.664106e+05</td>\n",
       "      <td>6.773957e+06</td>\n",
       "      <td>1.087289e+05</td>\n",
       "      <td>4.196250e+07</td>\n",
       "      <td>608.790698</td>\n",
       "      <td>9.190650e+05</td>\n",
       "      <td>41.232558</td>\n",
       "      <td>1.668049e+05</td>\n",
       "      <td>-1.140475e+06</td>\n",
       "      <td>1.470361e+06</td>\n",
       "      <td>64.895349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.329899</td>\n",
       "      <td>2.716369e+06</td>\n",
       "      <td>2582.700981</td>\n",
       "      <td>5.161930e+06</td>\n",
       "      <td>2.906172e+07</td>\n",
       "      <td>3.106201e+07</td>\n",
       "      <td>1.071333e+07</td>\n",
       "      <td>1.251828e+07</td>\n",
       "      <td>1178.317641</td>\n",
       "      <td>4.201494e+06</td>\n",
       "      <td>3.895777e+07</td>\n",
       "      <td>5.335348e+05</td>\n",
       "      <td>4.708321e+07</td>\n",
       "      <td>1841.033949</td>\n",
       "      <td>4.589253e+06</td>\n",
       "      <td>100.073111</td>\n",
       "      <td>3.198914e+05</td>\n",
       "      <td>4.025406e+06</td>\n",
       "      <td>5.942759e+06</td>\n",
       "      <td>86.979244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.770000e+02</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>-1.025000e+05</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>3.285000e+03</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.576788e+06</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>4.000000e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.285000e+03</td>\n",
       "      <td>-2.799289e+07</td>\n",
       "      <td>6.922300e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.670423e+07</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>3.208340e+07</td>\n",
       "      <td>3.098866e+08</td>\n",
       "      <td>3.117640e+08</td>\n",
       "      <td>9.734362e+07</td>\n",
       "      <td>1.303223e+08</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>1.545629e+07</td>\n",
       "      <td>4.345095e+08</td>\n",
       "      <td>5.235198e+06</td>\n",
       "      <td>8.392500e+07</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>4.266759e+07</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>1.398517e+06</td>\n",
       "      <td>-8.330000e+02</td>\n",
       "      <td>4.852193e+07</td>\n",
       "      <td>528.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              poi        salary   to_messages  deferral_payments  \\\n",
       "count  146.000000  9.500000e+01     86.000000       3.900000e+01   \n",
       "mean     0.123288  5.621943e+05   2073.860465       1.642674e+06   \n",
       "std      0.329899  2.716369e+06   2582.700981       5.161930e+06   \n",
       "min      0.000000  4.770000e+02     57.000000      -1.025000e+05   \n",
       "25%      0.000000           NaN           NaN                NaN   \n",
       "50%      0.000000           NaN           NaN                NaN   \n",
       "75%      0.000000           NaN           NaN                NaN   \n",
       "max      1.000000  2.670423e+07  15149.000000       3.208340e+07   \n",
       "\n",
       "       total_payments  exercised_stock_options         bonus  \\\n",
       "count    1.250000e+02             1.020000e+02  8.200000e+01   \n",
       "mean     5.081526e+06             5.987054e+06  2.374235e+06   \n",
       "std      2.906172e+07             3.106201e+07  1.071333e+07   \n",
       "min      1.480000e+02             3.285000e+03  7.000000e+04   \n",
       "25%               NaN                      NaN           NaN   \n",
       "50%               NaN                      NaN           NaN   \n",
       "75%               NaN                      NaN           NaN   \n",
       "max      3.098866e+08             3.117640e+08  9.734362e+07   \n",
       "\n",
       "       restricted_stock  shared_receipt_with_poi  restricted_stock_deferred  \\\n",
       "count      1.100000e+02                86.000000               1.800000e+01   \n",
       "mean       2.321741e+06              1176.465116               1.664106e+05   \n",
       "std        1.251828e+07              1178.317641               4.201494e+06   \n",
       "min       -2.604490e+06                 2.000000              -7.576788e+06   \n",
       "25%                 NaN                      NaN                        NaN   \n",
       "50%                 NaN                      NaN                        NaN   \n",
       "75%                 NaN                      NaN                        NaN   \n",
       "max        1.303223e+08              5521.000000               1.545629e+07   \n",
       "\n",
       "       total_stock_value      expenses  loan_advances  from_messages  \\\n",
       "count       1.260000e+02  9.500000e+01   4.000000e+00      86.000000   \n",
       "mean        6.773957e+06  1.087289e+05   4.196250e+07     608.790698   \n",
       "std         3.895777e+07  5.335348e+05   4.708321e+07    1841.033949   \n",
       "min        -4.409300e+04  1.480000e+02   4.000000e+05      12.000000   \n",
       "25%                  NaN           NaN            NaN            NaN   \n",
       "50%                  NaN           NaN            NaN            NaN   \n",
       "75%                  NaN           NaN            NaN            NaN   \n",
       "max         4.345095e+08  5.235198e+06   8.392500e+07   14368.000000   \n",
       "\n",
       "              other  from_this_person_to_poi  director_fees  deferred_income  \\\n",
       "count  9.300000e+01                86.000000   1.700000e+01     4.900000e+01   \n",
       "mean   9.190650e+05                41.232558   1.668049e+05    -1.140475e+06   \n",
       "std    4.589253e+06               100.073111   3.198914e+05     4.025406e+06   \n",
       "min    2.000000e+00                 0.000000   3.285000e+03    -2.799289e+07   \n",
       "25%             NaN                      NaN            NaN              NaN   \n",
       "50%             NaN                      NaN            NaN              NaN   \n",
       "75%             NaN                      NaN            NaN              NaN   \n",
       "max    4.266759e+07               609.000000   1.398517e+06    -8.330000e+02   \n",
       "\n",
       "       long_term_incentive  from_poi_to_this_person  \n",
       "count         6.600000e+01                86.000000  \n",
       "mean          1.470361e+06                64.895349  \n",
       "std           5.942759e+06                86.979244  \n",
       "min           6.922300e+04                 0.000000  \n",
       "25%                    NaN                      NaN  \n",
       "50%                    NaN                      NaN  \n",
       "75%                    NaN                      NaN  \n",
       "max           4.852193e+07               528.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#create an overview of the dataset\n",
    "#skeleton code from poi_id.py, except that I build a DataFrame to ease exploration\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "with open(\"final_project_dataset.pkl\",'r') as data:\n",
    "    data_dict= pickle.load(data)\n",
    "\n",
    "#features_list: the name list of all features except 'email_address'(since it can't be computed)   \n",
    "all_features= data_dict['TOTAL'].keys()\n",
    "all_features.pop(all_features.index(\"poi\"))\n",
    "all_features.pop(all_features.index(\"email_address\"))\n",
    "features_list=[\"poi\",]+all_features\n",
    "\n",
    "#df: the data frame holding every data point from my_dataset \n",
    "#with each data point as a row and each feature (19 features + 1 label) as a column\n",
    "#notice that in df all NaNs are saved\n",
    "my_dataset = data_dict\n",
    "data = featureFormat(my_dataset, features_list,remove_NaN=False, remove_all_zeroes=False, sort_keys = False)\n",
    "df=pd.DataFrame(data,index=data_dict.keys(),columns=features_list)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Dataset Background\n",
    "[Enron Corpus](https://en.wikipedia.org/wiki/Enron_Corpus) is acquired by Federal Energy Regulatory Commission during its investigation towards the company's collapse. This dataset is later made publicly accessible and widely studied by computer scientists. \n",
    "For this project we are using the lastest May 7,2015 version of it. Our project designer Katie Malone scraped 6 important email features along with 13 ecomonic features from 146 subjects and ensembled them into a dictionary container with 18 of subjects manually labeled  as 'POI'.   \n",
    "\n",
    "#### 1.4 How Background Understanding helps\n",
    "Several big figures are found involved most in the crime, such as Enron ex-Chairman Kenneth L. Lay, ex-CEO Jeffrey K. Skilling and ex-CFO Andrew S. Fastow. I also notice a report that Lou Lung Pai, the ex-head of EES(Enron Energy Services), made 300 million dolloars by selling all his Enron stock right before the collapse. This guy finally got away from trial by a coincident divorce. Besides, Kenneth D. Rice, Greg Whalley, Rebecca Mark, Ben F. Glisan Jr., Mark E. Koenig are also accused of misconduct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HANNON KEVIN P          1.0\n",
       "COLWELL WESLEY          1.0\n",
       "RIEKER PAULA H          1.0\n",
       "KOPPER MICHAEL J        1.0\n",
       "SHELBY REX              1.0\n",
       "DELAINEY DAVID W        1.0\n",
       "LAY KENNETH L           1.0\n",
       "BOWEN JR RAYMOND M      1.0\n",
       "BELDEN TIMOTHY N        1.0\n",
       "FASTOW ANDREW S         1.0\n",
       "CALGER CHRISTOPHER F    1.0\n",
       "RICE KENNETH D          1.0\n",
       "SKILLING JEFFREY K      1.0\n",
       "YEAGER F SCOTT          1.0\n",
       "HIRKO JOSEPH            1.0\n",
       "KOENIG MARK E           1.0\n",
       "CAUSEY RICHARD A        1.0\n",
       "GLISAN JR BEN F         1.0\n",
       "Name: poi, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.poi[df.poi==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proofreadinging familiar names one by one, I noticed Greg Whalley, Rebecca Mark, and Lou Lung Pai aren't shown here. Dig deeper,  I found Greg and Rebecca somehow are simply not included in this dataset. While 'LOU PAI L' is labeled as non-POI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                                 0.0\n",
       "salary                         261879.0\n",
       "to_messages                         NaN\n",
       "deferral_payments                   NaN\n",
       "total_payments                3123383.0\n",
       "exercised_stock_options      15364167.0\n",
       "bonus                         1000000.0\n",
       "restricted_stock              8453763.0\n",
       "shared_receipt_with_poi             NaN\n",
       "restricted_stock_deferred           NaN\n",
       "total_stock_value            23817930.0\n",
       "expenses                        32047.0\n",
       "loan_advances                       NaN\n",
       "from_messages                       NaN\n",
       "other                         1829457.0\n",
       "from_this_person_to_poi             NaN\n",
       "director_fees                       NaN\n",
       "deferred_income                     NaN\n",
       "long_term_incentive                 NaN\n",
       "from_poi_to_this_person             NaN\n",
       "Name: PAI LOU L, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix['PAI LOU L']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this guy is surely not innocent! Maybe Katie simply missed this guy out in manual labeling, or she labeled only persons with conviction. But labeling Lou Lung Pai as non-POI is very misleading since this guy is in fact a accomplice.  **So I change his 'poi' value as 1. Considering the very imbalanced classes (18 poi vs 128 non-poi), the correction is very helpful. Now poi versus non-poi is 19:127. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix['PAI LOU L','poi']=1;df.ix['PAI LOU L','poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALLEN PHILLIP K',\n",
       " 'BADUM JAMES P',\n",
       " 'BANNANTINE JAMES M',\n",
       " 'BAXTER JOHN C',\n",
       " 'BAY FRANKLIN R',\n",
       " 'BAZELIDES PHILIP J',\n",
       " 'BECK SALLY W',\n",
       " 'BELDEN TIMOTHY N',\n",
       " 'BELFER ROBERT',\n",
       " 'BERBERIAN DAVID',\n",
       " 'BERGSIEKER RICHARD P',\n",
       " 'BHATNAGAR SANJAY',\n",
       " 'BIBI PHILIPPE A',\n",
       " 'BLACHMAN JEREMY M',\n",
       " 'BLAKE JR. NORMAN P',\n",
       " 'BOWEN JR RAYMOND M',\n",
       " 'BROWN MICHAEL',\n",
       " 'BUCHANAN HAROLD G',\n",
       " 'BUTTS ROBERT H',\n",
       " 'BUY RICHARD B',\n",
       " 'CALGER CHRISTOPHER F',\n",
       " 'CARTER REBECCA C',\n",
       " 'CAUSEY RICHARD A',\n",
       " 'CHAN RONNIE',\n",
       " 'CHRISTODOULOU DIOMEDES',\n",
       " 'CLINE KENNETH W',\n",
       " 'COLWELL WESLEY',\n",
       " 'CORDES WILLIAM R',\n",
       " 'COX DAVID',\n",
       " 'CUMBERLAND MICHAEL S',\n",
       " 'DEFFNER JOSEPH M',\n",
       " 'DELAINEY DAVID W',\n",
       " 'DERRICK JR. JAMES V',\n",
       " 'DETMERING TIMOTHY J',\n",
       " 'DIETRICH JANET R',\n",
       " 'DIMICHELE RICHARD G',\n",
       " 'DODSON KEITH',\n",
       " 'DONAHUE JR JEFFREY M',\n",
       " 'DUNCAN JOHN H',\n",
       " 'DURAN WILLIAM D',\n",
       " 'ECHOLS JOHN B',\n",
       " 'ELLIOTT STEVEN',\n",
       " 'FALLON JAMES B',\n",
       " 'FASTOW ANDREW S',\n",
       " 'FITZGERALD JAY L',\n",
       " 'FOWLER PEGGY',\n",
       " 'FOY JOE',\n",
       " 'FREVERT MARK A',\n",
       " 'FUGH JOHN L',\n",
       " 'GAHN ROBERT S',\n",
       " 'GARLAND C KEVIN',\n",
       " 'GATHMANN WILLIAM D',\n",
       " 'GIBBS DANA R',\n",
       " 'GILLIS JOHN',\n",
       " 'GLISAN JR BEN F',\n",
       " 'GOLD JOSEPH',\n",
       " 'GRAMM WENDY L',\n",
       " 'GRAY RODNEY',\n",
       " 'HAEDICKE MARK E',\n",
       " 'HANNON KEVIN P',\n",
       " 'HAUG DAVID L',\n",
       " 'HAYES ROBERT E',\n",
       " 'HAYSLETT RODERICK J',\n",
       " 'HERMANN ROBERT J',\n",
       " 'HICKERSON GARY J',\n",
       " 'HIRKO JOSEPH',\n",
       " 'HORTON STANLEY C',\n",
       " 'HUGHES JAMES A',\n",
       " 'HUMPHREY GENE E',\n",
       " 'IZZO LAWRENCE L',\n",
       " 'JACKSON CHARLENE R',\n",
       " 'JAEDICKE ROBERT',\n",
       " 'KAMINSKI WINCENTY J',\n",
       " 'KEAN STEVEN J',\n",
       " 'KISHKILL JOSEPH G',\n",
       " 'KITCHEN LOUISE',\n",
       " 'KOENIG MARK E',\n",
       " 'KOPPER MICHAEL J',\n",
       " 'LAVORATO JOHN J',\n",
       " 'LAY KENNETH L',\n",
       " 'LEFF DANIEL P',\n",
       " 'LEMAISTRE CHARLES',\n",
       " 'LEWIS RICHARD',\n",
       " 'LINDHOLM TOD A',\n",
       " 'LOCKHART EUGENE E',\n",
       " 'LOWRY CHARLES P',\n",
       " 'MARTIN AMANDA K',\n",
       " 'MCCARTY DANNY J',\n",
       " 'MCCLELLAN GEORGE',\n",
       " 'MCCONNELL MICHAEL S',\n",
       " 'MCDONALD REBECCA',\n",
       " 'MCMAHON JEFFREY',\n",
       " 'MENDELSOHN JOHN',\n",
       " 'METTS MARK',\n",
       " 'MEYER JEROME J',\n",
       " 'MEYER ROCKFORD G',\n",
       " 'MORAN MICHAEL P',\n",
       " 'MORDAUNT KRISTINA M',\n",
       " 'MULLER MARK S',\n",
       " 'MURRAY JULIA H',\n",
       " 'NOLES JAMES L',\n",
       " 'OLSON CINDY K',\n",
       " 'OVERDYKE JR JERE C',\n",
       " 'PAI LOU L',\n",
       " 'PEREIRA PAULO V. FERRAZ',\n",
       " 'PICKERING MARK R',\n",
       " 'PIPER GREGORY F',\n",
       " 'PIRO JIM',\n",
       " 'POWERS WILLIAM',\n",
       " 'PRENTICE JAMES',\n",
       " 'REDMOND BRIAN L',\n",
       " 'REYNOLDS LAWRENCE',\n",
       " 'RICE KENNETH D',\n",
       " 'RIEKER PAULA H',\n",
       " 'SAVAGE FRANK',\n",
       " 'SCRIMSHAW MATTHEW',\n",
       " 'SHANKMAN JEFFREY A',\n",
       " 'SHAPIRO RICHARD S',\n",
       " 'SHARP VICTORIA T',\n",
       " 'SHELBY REX',\n",
       " 'SHERRICK JEFFREY B',\n",
       " 'SHERRIFF JOHN R',\n",
       " 'SKILLING JEFFREY K',\n",
       " 'STABLER FRANK',\n",
       " 'SULLIVAN-SHAKLOVITZ COLLEEN',\n",
       " 'SUNDE MARTIN',\n",
       " 'TAYLOR MITCHELL S',\n",
       " 'THE TRAVEL AGENCY IN THE PARK',\n",
       " 'THORN TERENCE H',\n",
       " 'TILNEY ELIZABETH A',\n",
       " 'TOTAL',\n",
       " 'UMANOFF ADAM S',\n",
       " 'URQUHART JOHN A',\n",
       " 'WAKEHAM JOHN',\n",
       " 'WALLS JR ROBERT H',\n",
       " 'WALTERS GARETH W',\n",
       " 'WASAFF GEORGE',\n",
       " 'WESTFAHL RICHARD K',\n",
       " 'WHALEY DAVID A',\n",
       " 'WHALLEY LAWRENCE G',\n",
       " 'WHITE JR THOMAS E',\n",
       " 'WINOKUR JR. HERBERT S',\n",
       " 'WODRASKA JOHN',\n",
       " 'WROBEL BRUCE',\n",
       " 'YEAGER F SCOTT',\n",
       " 'YEAP SOON']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I inspect the row names about any suspicious. I soon find suspicious names such as 'TOTAL','THE TRAVEL AGENCY IN THE PARK', and 'YEAP SOON', which for me doesn't sounds like normal Western names. So I display them and find 'TOTAL' is invalid as it has extremely large value comparing with medians (from df.describe() above), this might lead from entry error. 'THE TRAVEL AGENCY IN THE PARK' is surely not a valid emloyee name. 'YEAP SOON' ,although unusual, is actually valid name.  So I removed the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "#now poi versus non-poi is 19:125\n",
    "data_dict.pop('TOTAL')\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "df.drop(['TOTAL','THE TRAVEL AGENCY IN THE PARK'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Feature Scaling and Selection  \n",
    "\n",
    ">*What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 New features engineered\n",
    ">**Concern about sign** \n",
    "\n",
    "I find all the valid deferred_income and restricted_stock_deferred values is negative, it seems Katie treat them as the owner's liability instead of wealth. I think it makes sense for deferred income ('deferred' items baiscally describes trades based on credence, in which services/goods is not delivered at the same time of payment.) However it might be inappropriate for restricted_stock_deferred. But since sign won't affect our analysis, I decide to leave them just as they are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding one: ambition** \n",
    "\n",
    "I searched for difference between POI and non-POI by comparing the medians (avoid impact from extreme values) of features. It turns out POIs favor high risks and high profitable approaches in managng money, such as loan_advances,bonus, total_stock_value and long_term_incentive. While both group have comparable salaries and expenses.  \n",
    "\n",
    "**Finding two: poi_network**  \n",
    "\n",
    "Another interesting finding is that POIs group scores higher in all email related features no matter it's from or to message. It is  especially marked when this feature is poi related ('from_poi_to_this_person','from_this_person_to_poi','shared_receipt_with_poi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>expenses</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>250877.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>260455.0</td>\n",
       "      <td>1056092.5</td>\n",
       "      <td>985293.0</td>\n",
       "      <td>700000.0</td>\n",
       "      <td>409554.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>-140264.0</td>\n",
       "      <td>1022417.0</td>\n",
       "      <td>46145.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10221.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>106164.5</td>\n",
       "      <td>-121284.0</td>\n",
       "      <td>375304.0</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>276788.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>202911.0</td>\n",
       "      <td>1868758.0</td>\n",
       "      <td>5538001.0</td>\n",
       "      <td>1250000.0</td>\n",
       "      <td>1116675.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2493616.0</td>\n",
       "      <td>46950.0</td>\n",
       "      <td>81525000.0</td>\n",
       "      <td>44.5</td>\n",
       "      <td>150458.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-262500.0</td>\n",
       "      <td>1134637.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       salary  to_messages  deferral_payments  total_payments  \\\n",
       "poi                                                             \n",
       "0.0  250877.0        944.0           260455.0       1056092.5   \n",
       "1.0  276788.0       1875.0           202911.0       1868758.0   \n",
       "\n",
       "     exercised_stock_options      bonus  restricted_stock  \\\n",
       "poi                                                         \n",
       "0.0                 985293.0   700000.0          409554.0   \n",
       "1.0                5538001.0  1250000.0         1116675.0   \n",
       "\n",
       "     shared_receipt_with_poi  restricted_stock_deferred  total_stock_value  \\\n",
       "poi                                                                          \n",
       "0.0                    594.0                  -140264.0          1022417.0   \n",
       "1.0                   1589.0                        NaN          2493616.0   \n",
       "\n",
       "     expenses  loan_advances  from_messages     other  \\\n",
       "poi                                                     \n",
       "0.0   46145.0      1200000.0           41.0   10221.5   \n",
       "1.0   46950.0     81525000.0           44.5  150458.0   \n",
       "\n",
       "     from_this_person_to_poi  director_fees  deferred_income  \\\n",
       "poi                                                            \n",
       "0.0                      6.0       106164.5        -121284.0   \n",
       "1.0                     15.5            NaN        -262500.0   \n",
       "\n",
       "     long_term_incentive  from_poi_to_this_person  \n",
       "poi                                                \n",
       "0.0             375304.0                     26.5  \n",
       "1.0            1134637.0                     62.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('poi').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concern about overfitting**\n",
    "Considering the small size of dataset (144 effective points only), I think only a small number of features should be finally used to avoid overfitting (as small dataset with lots of features is very vulnerable to overfiting). I decide to reduce features number while save most possible infomation. PCA is a handy tool for that. And in last section I noticed lots of NaN in some fields. Intuitively I think features suffering from big chunk of data loss should be good candidates for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                            0\n",
       "total_stock_value             19\n",
       "total_payments                21\n",
       "restricted_stock              35\n",
       "exercised_stock_options       43\n",
       "salary                        50\n",
       "expenses                      50\n",
       "other                         53\n",
       "from_this_person_to_poi       58\n",
       "from_messages                 58\n",
       "from_poi_to_this_person       58\n",
       "shared_receipt_with_poi       58\n",
       "to_messages                   58\n",
       "bonus                         63\n",
       "long_term_incentive           79\n",
       "deferred_income               96\n",
       "deferral_payments            106\n",
       "restricted_stock_deferred    127\n",
       "director_fees                128\n",
       "loan_advances                141\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Engineered Feature One : Ambition**  \n",
    "Bonus, loan_advances and long_term_incentive are chosen to engineer Ambition. I didn't perform scaling since the units and values of three features are comparable. The old features lack infomation individually. While 'ambition' extract 97% of the total infomation. Hopefully the new feature will give us surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96631851])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nnz_df: a dataframe similar with df except it is without empty values\n",
    "nnz_df=df.fillna(0)\n",
    "\n",
    "# 'ambition' : created from 3 original economic features\n",
    "bonus_loan_incentive=nnz_df[['bonus','long_term_incentive','loan_advances']]\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=1)\n",
    "nnz_df['ambition']=pca.fit_transform(bonus_loan_incentive.values,nnz_df.poi.values)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Engineered Feature two and three:  to_poi_fraction,  from_poi_fraction**  \n",
    "to_poi_fraction = from_this_person_to_poi / from_messages   \n",
    "from_poi_fraction = from_poi_to_this_person / to_messages    \n",
    "The new variable is more informative in telling about how POIs  weighes in this individual's network. I think they should be more powerful than the previous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create new email feature 'to_poi_fraction'\n",
    "#'to_poi_fraction'='from_this_person_to_poi'/'from_messages'\n",
    "poi_toall=nnz_df[['from_this_person_to_poi','from_messages']] \n",
    "nnz_df['to_poi_fraction']=0\n",
    "for idx,row in zip(poi_toall.index,poi_toall.values):\n",
    "    if row[0]!=0 and row[1]!=0:\n",
    "        nnz_df.ix[idx,'to_poi_fraction']=row[0]/row[1]*100\n",
    "        \n",
    "#create new email feature 'from_poi_fraction'\n",
    "#'from_poi_fraction'='from_poi_to_this_person'/'to_messages'\n",
    "poi_fromall=nnz_df[['from_poi_to_this_person','to_messages']] \n",
    "nnz_df['from_poi_fraction']=0\n",
    "for idx,row in zip(poi_fromall.index,poi_fromall.values):\n",
    "    if row[0]!=0 and row[1]!=0:\n",
    "        nnz_df.ix[idx,'from_poi_fraction']=row[0]/row[1]*100\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Features Used\n",
    "After trying twenty like kinds of different conditions, I found all the three engineered features are very **powerful under some circumstances**.  Their predictive power  depends a lot on what kind of feature scaling (log10 transformation, normalization or both) is done and what features are chosen for scaling before selection. Nonetheless the scores of Ambition and from_poi_fraction are always among the best 10,  **and to_poi_fraction is even more powerful than the two** (Once log10 transformation and normalization are applied, to_poi_fraction always appear as top3 out of the 22. In contrast it scored very low if preprocessing not conducted. )   \n",
    "**The final features_list has only 4 features, including one engineered feature, to_poi_fraction.**  The rest are others, expenses and bonus respectively.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Feature Scaling \n",
    "**log10 transformation and normalization**  \n",
    "\n",
    "I have 22 features(19 original + 3 engineered), and one label ('POI'). Since distribution of some of features is so skewed while many algorithems (such as RBF kernel of support vector machines) assume the features are normally distributed and all features have variance in the same order, I plan to first conduct feature scaling  before feature selection. \n",
    "To avoid repeatitive work I decide to write pipeline functions and observe how my models respond.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import log10\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "def preprocess(k,translist,escape=[],lg=True):\n",
    "    #return normalized and log transformed values of input columns\n",
    "    def norm_log10_scaling(flist):\n",
    "        temp=nnz_df[flist].copy()\n",
    "        temp=abs(temp+1)\n",
    "        if lg:\n",
    "            for cln in flist:\n",
    "                temp[cln]=temp[cln].apply(log10)\n",
    "        temp=normalize(temp.values,axis=0)\n",
    "        temp=pd.DataFrame(temp,columns=flist,index=nnz_df.index)\n",
    "        return temp\n",
    "    \n",
    "    #input a feature dataframe, return a dataframe with selected features\n",
    "    def kbest(df,k):\n",
    "        slb=SelectKBest(k=k)\n",
    "    \n",
    "        #get values of the best k features\n",
    "        best_features=slb.fit_transform(df.values,nnz_df.poi.values)\n",
    "    \n",
    "        #get names of the best k features\n",
    "        scores=zip(df.columns,slb.scores_)\n",
    "        top_scored=sorted(scores,key=lambda tup: tup[1],reverse=True)[:k]    \n",
    "        best_names=[]    \n",
    "        for feature in df.columns:\n",
    "            if feature in [x for (x,y) in top_scored]:\n",
    "                best_names.append(feature)    \n",
    "        best=pd.DataFrame(best_features,columns=best_names,index=nnz_df.index) \n",
    "        return best,top_scored\n",
    "\n",
    "    #log transform feature except those in escape list\n",
    "    df=nnz_df[features_list].copy()\n",
    "    flist=[x for x in features_list if x not in escape]\n",
    "    if flist:\n",
    "        transformed=norm_log10_scaling(flist)    \n",
    "        escaped=df[escape].copy()\n",
    "        df=pd.DataFrame(np.concatenate((transformed,escaped),axis=1),index=nnz_df.index,columns=transformed.columns+escaped.columns)\n",
    "\n",
    "    return kbest(df,k=k)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Feature Selection\n",
    "\n",
    "**SelectKBest**  \n",
    "\n",
    "For feature selection I used SelectKBest ( in the kbest functionality). To observe the response of the 7 chosen models I change the input features from very small numbers,3 (i.e., very strict feature filtering), to 5,10,15,and untile 22 (i.e., less strict filtering to no filtering ), I see first a gradual increase in overall performance of 7 models and then decrease, with the peak appearing at somewhere middle depends on feature scaling conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "from collections import defaultdict\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#to preprocess features accordinly, separate eco_features with email_features\n",
    "#create name list for both classes\n",
    "eco_features_list=[ u'salary', u'deferral_payments',\\\n",
    "       u'total_payments', u'exercised_stock_options', u'bonus',\\\n",
    "       u'restricted_stock', u'restricted_stock_deferred', u'total_stock_value', u'expenses',\\\n",
    "       u'loan_advances',  u'other',u'director_fees', u'deferred_income',u'long_term_incentive',u'ambition']\n",
    "\n",
    "email_features_list=[u'to_messages',u'shared_receipt_with_poi',u'from_messages',u'from_this_person_to_poi',\\\n",
    "                     u'from_poi_to_this_person',u'to_poi_fraction', u'from_poi_fraction']\n",
    "\n",
    "features_list=eco_features_list+email_features_list\n",
    "labels=nnz_df.poi.values\n",
    "\n",
    "log_diary=defaultdict(lambda:{})\n",
    "\n",
    "def models_comparison(k,translist=features_list,escape=[],lg=True,n_iter=100,scoring='recall',aver='micro'):\n",
    "    #log transform and normalize all features except those in escape \n",
    "    best,top_scored=preprocess(k=k,translist=translist,escape=escape,lg=lg)\n",
    "    print '========Top {} features and their scores:'.format(k)\n",
    "    for feature,score in top_scored:\n",
    "        print '%-30s %s'%(feature,score)\n",
    "    print    \n",
    "    features=best.values\n",
    "    \n",
    "    def clf_score(clf,n_iter=n_iter,log=\"\"):\n",
    "        cv = StratifiedShuffleSplit(labels, n_iter=n_iter, random_state = 42)           \n",
    "        results={'precision':[],'recall':[],'fscore':[],'accuracy':[]}\n",
    "        for traincv, testcv in cv:\n",
    "                clf.fit(features[traincv], labels[traincv])\n",
    "                pred=clf.predict(features[testcv])\n",
    "                prfs=precision_recall_fscore_support(labels[testcv],pred,average=aver)\n",
    "                results['precision'].append(prfs[0])\n",
    "                results['recall'].append(prfs[1])\n",
    "                results['fscore'].append(prfs[2])\n",
    "                results['accuracy'].append(accuracy_score(labels[testcv],pred))\n",
    "                \n",
    "        for k,v in results.iteritems():\n",
    "            average=sum(v)/float(len(v))\n",
    "            results[k]=average\n",
    "    \n",
    "        if log:\n",
    "            log_diary[log]=results                                       \n",
    "        return results\n",
    "\n",
    "            \n",
    "    def clf_best_params(clf,parameters,scoring=scoring):\n",
    "        gd=GridSearchCV(clf,parameters,scoring=scoring)\n",
    "        gd.fit(features,labels)\n",
    "        return gd    \n",
    "    \n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb=GaussianNB()\n",
    "    clf_score(nb,log='GaussianNB')\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    sv=SVC(kernel='rbf',C=1,random_state=42)\n",
    "    parameters={'C':[1,10,1000,100000]}\n",
    "    sv=clf_best_params(sv,parameters)\n",
    "    clf_score(sv,log='rbf_SVC')\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression \n",
    "    clf=LogisticRegression(random_state=42)\n",
    "    clf_score(clf,log='LogitRegression')\n",
    "\n",
    "    from sklearn import tree\n",
    "    clf=tree.DecisionTreeClassifier(random_state=42)\n",
    "    clf=clf_best_params(clf,parameters={'min_samples_split':[5,10,20,40]})\n",
    "    clf_score(clf,log='DecisionTree')\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rfc=RandomForestClassifier(random_state=42)\n",
    "    parameters={'n_estimators':[1,2,5,10],'min_samples_split':[5,10,20]}\n",
    "    rfc=clf_best_params(rfc,parameters)\n",
    "    clf_score(rfc,log='RandomForest')\n",
    "    \n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    adc=AdaBoostClassifier(random_state=42)\n",
    "    parameters={'n_estimators':[1,2,5,10]}\n",
    "    adc=clf_best_params(adc,parameters)\n",
    "    clf_score(adc,log='AdaBoost')\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn=KNeighborsClassifier()\n",
    "    parameters={'n_neighbors':[1,2,5,10]}\n",
    "    knn=clf_best_params(knn,parameters)\n",
    "    clf_score(knn,log=\"KNeighbors\")\n",
    "    \n",
    "    with open('my_classifier.pkl', \"w\") as clf_outfile:\n",
    "        pickle.dump(nb, clf_outfile)\n",
    "\n",
    "    log_sheet=pd.DataFrame.from_dict(log_diary,orient='index')\n",
    "    return log_sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**economic features and email features**  \n",
    ">Numbers of economic features are generally very big (1e5~1e7), while email numbers are mostly less than one thousand. Besides when I check the histogram distribution of them individually, I found all original economic features are at first right skewed, and a simple log10 scaling can drag them back to a symetric shape. (Except 'ambition', the 'unatural' economic feature.) However email features are distributed very very sparsely and extremely skewed, one log10 transformation can't work as effectively as economic features. My first thought is economic and email features are supposed to be treated differently, but the results turn to be different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When escape=features_list\n",
      "========Top 22 features and their scores:\n",
      "total_stock_value              34.9075976019\n",
      "exercised_stock_options        33.2879034061\n",
      "bonus                          20.6883364652\n",
      "salary                         18.6050424233\n",
      "restricted_stock               16.2334432323\n",
      "to_poi_fraction                14.4941026933\n",
      "deferred_income                10.4590114322\n",
      "long_term_incentive            8.78693453741\n",
      "total_payments                 8.6034931369\n",
      "shared_receipt_with_poi        7.35815751949\n",
      "ambition                       7.08376246925\n",
      "loan_advances                  6.78241410792\n",
      "expenses                       5.848549417\n",
      "other                          5.57983168439\n",
      "from_poi_to_this_person        4.50604520117\n",
      "from_poi_fraction              2.53715167747\n",
      "director_fees                  2.24467820956\n",
      "from_this_person_to_poi        2.08554158894\n",
      "to_messages                    1.28922024446\n",
      "deferral_payments              0.278481478789\n",
      "from_messages                  0.209348310101\n",
      "restricted_stock_deferred      0.0691453328512\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.305   0.404833  0.327381  0.864000\n",
      "DecisionTree      0.405   0.345667  0.352881  0.835333\n",
      "GaussianNB        0.845   0.149287  0.252830  0.340000\n",
      "KNeighbors        0.355   0.386667  0.350333  0.848000\n",
      "LogitRegression   0.280   0.308167  0.278048  0.831333\n",
      "RandomForest      0.220   0.217595  0.205746  0.826000\n",
      "rbf_SVC           0.000   0.000000  0.000000  0.866667\n",
      "\n",
      "When escape=None\n",
      "========Top 22 features and their scores:\n",
      "other                          19.6575366659\n",
      "to_poi_fraction                16.6636560959\n",
      "expenses                       14.9631978528\n",
      "bonus                          12.9509369506\n",
      "salary                         10.3048634375\n",
      "from_poi_to_this_person        8.35166246726\n",
      "from_this_person_to_poi        8.089761319\n",
      "total_stock_value              7.45637342328\n",
      "total_payments                 7.31584157833\n",
      "restricted_stock               7.16702125727\n",
      "deferred_income                6.79178398167\n",
      "from_poi_fraction              6.16052678785\n",
      "shared_receipt_with_poi        4.91404733638\n",
      "long_term_incentive            3.76155201143\n",
      "to_messages                    3.07016516401\n",
      "restricted_stock_deferred      2.90095588193\n",
      "director_fees                  2.72707461177\n",
      "loan_advances                  1.86899960774\n",
      "from_messages                  0.908476428726\n",
      "exercised_stock_options        0.304584036906\n",
      "ambition                       0.136286353093\n",
      "deferral_payments              0.0374545946982\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.315   0.391500  0.326714  0.863333\n",
      "DecisionTree      0.420   0.363333  0.369952  0.841333\n",
      "GaussianNB        0.890   0.264493  0.402366  0.636667\n",
      "KNeighbors        0.440   0.395000  0.387524  0.837333\n",
      "LogitRegression   0.000   0.000000  0.000000  0.866667\n",
      "RandomForest      0.225   0.238786  0.213857  0.824667\n",
      "rbf_SVC           0.385   0.356833  0.347190  0.844000\n",
      "\n",
      "When escape=email_features_list\n",
      "========Top 22 features and their scores:\n",
      "from_this_person_to_poi        19.6575366659\n",
      "from_poi_fraction              14.9631978528\n",
      "total_payments                 14.4941026933\n",
      "director_fees                  12.9509369506\n",
      "ambition                       10.3048634375\n",
      "from_messages                  7.45637342328\n",
      "salary                         7.35815751949\n",
      "deferral_payments              7.31584157833\n",
      "exercised_stock_options        7.16702125727\n",
      "long_term_incentive            6.79178398167\n",
      "to_poi_fraction                4.50604520117\n",
      "other                          3.76155201143\n",
      "expenses                       2.90095588193\n",
      "loan_advances                  2.72707461177\n",
      "total_stock_value              2.53715167747\n",
      "to_messages                    2.08554158894\n",
      "from_poi_to_this_person        1.86899960774\n",
      "restricted_stock_deferred      1.28922024446\n",
      "deferred_income                0.304584036906\n",
      "shared_receipt_with_poi        0.209348310101\n",
      "restricted_stock               0.136286353093\n",
      "bonus                          0.0374545946982\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.315   0.391500  0.326714  0.863333\n",
      "DecisionTree      0.420   0.363333  0.369952  0.841333\n",
      "GaussianNB        0.825   0.246572  0.374724  0.632000\n",
      "KNeighbors        0.310   0.289167  0.279619  0.818000\n",
      "LogitRegression   0.110   0.138333  0.117667  0.842667\n",
      "RandomForest      0.225   0.238786  0.213857  0.826667\n",
      "rbf_SVC           0.085   0.024757  0.035261  0.810000\n",
      "\n",
      "When escape=eco_features_list\n",
      "========Top 22 features and their scores:\n",
      "restricted_stock               34.9075976019\n",
      "from_this_person_to_poi        33.2879034061\n",
      "loan_advances                  20.6883364652\n",
      "from_messages                  18.6050424233\n",
      "exercised_stock_options        16.6636560959\n",
      "long_term_incentive            16.2334432323\n",
      "to_poi_fraction                10.4590114322\n",
      "total_payments                 8.78693453741\n",
      "from_poi_to_this_person        8.6034931369\n",
      "director_fees                  8.35166246726\n",
      "deferred_income                8.089761319\n",
      "total_stock_value              7.08376246925\n",
      "salary                         6.78241410792\n",
      "expenses                       6.16052678785\n",
      "restricted_stock_deferred      5.848549417\n",
      "shared_receipt_with_poi        5.57983168439\n",
      "bonus                          4.91404733638\n",
      "ambition                       3.07016516401\n",
      "to_messages                    2.24467820956\n",
      "deferral_payments              0.908476428726\n",
      "from_poi_fraction              0.278481478789\n",
      "other                          0.0691453328512\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.305   0.404833  0.327381  0.864000\n",
      "DecisionTree      0.400   0.358167  0.355048  0.842667\n",
      "GaussianNB        0.900   0.156591  0.265840  0.335333\n",
      "KNeighbors        0.355   0.386667  0.350333  0.848000\n",
      "LogitRegression   0.290   0.386167  0.313524  0.855333\n",
      "RandomForest      0.275   0.279333  0.261238  0.834000\n",
      "rbf_SVC           0.000   0.000000  0.000000  0.866667\n"
     ]
    }
   ],
   "source": [
    "#Comparing with all features being treated equally, \n",
    "#only email features escape from transformation,\n",
    "#or only economic features escape from transformation,\n",
    "# gives no better feature scoring report or performace report.\n",
    "print \"\\nWhen escape=features_list\"\n",
    "print models_comparison(22,escape=features_list)\n",
    "print \"\\nWhen escape=None\"\n",
    "print models_comparison(22)\n",
    "print \"\\nWhen escape=email_features_list\"\n",
    "print models_comparison(22,escape=email_features_list)\n",
    "print \"\\nWhen escape=eco_features_list\"\n",
    "print models_comparison(22,escape=eco_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When  features selected= 22, escape=features_list\n",
      "========Top 22 features and their scores:\n",
      "total_stock_value              34.9075976019\n",
      "exercised_stock_options        33.2879034061\n",
      "bonus                          20.6883364652\n",
      "salary                         18.6050424233\n",
      "restricted_stock               16.2334432323\n",
      "to_poi_fraction                14.4941026933\n",
      "deferred_income                10.4590114322\n",
      "long_term_incentive            8.78693453741\n",
      "total_payments                 8.6034931369\n",
      "shared_receipt_with_poi        7.35815751949\n",
      "ambition                       7.08376246925\n",
      "loan_advances                  6.78241410792\n",
      "expenses                       5.848549417\n",
      "other                          5.57983168439\n",
      "from_poi_to_this_person        4.50604520117\n",
      "from_poi_fraction              2.53715167747\n",
      "director_fees                  2.24467820956\n",
      "from_this_person_to_poi        2.08554158894\n",
      "to_messages                    1.28922024446\n",
      "deferral_payments              0.278481478789\n",
      "from_messages                  0.209348310101\n",
      "restricted_stock_deferred      0.0691453328512\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.305   0.404833  0.327381  0.864000\n",
      "DecisionTree      0.405   0.345667  0.352881  0.835333\n",
      "GaussianNB        0.845   0.149287  0.252830  0.340000\n",
      "KNeighbors        0.355   0.386667  0.350333  0.848000\n",
      "LogitRegression   0.280   0.308167  0.278048  0.831333\n",
      "RandomForest      0.220   0.217595  0.205746  0.826000\n",
      "rbf_SVC           0.000   0.000000  0.000000  0.866667\n",
      "\n",
      "When features selected= 20, escape=features_list\n",
      "========Top 20 features and their scores:\n",
      "total_stock_value              34.9075976019\n",
      "exercised_stock_options        33.2879034061\n",
      "bonus                          20.6883364652\n",
      "salary                         18.6050424233\n",
      "restricted_stock               16.2334432323\n",
      "to_poi_fraction                14.4941026933\n",
      "deferred_income                10.4590114322\n",
      "long_term_incentive            8.78693453741\n",
      "total_payments                 8.6034931369\n",
      "shared_receipt_with_poi        7.35815751949\n",
      "ambition                       7.08376246925\n",
      "loan_advances                  6.78241410792\n",
      "expenses                       5.848549417\n",
      "other                          5.57983168439\n",
      "from_poi_to_this_person        4.50604520117\n",
      "from_poi_fraction              2.53715167747\n",
      "director_fees                  2.24467820956\n",
      "from_this_person_to_poi        2.08554158894\n",
      "to_messages                    1.28922024446\n",
      "deferral_payments              0.278481478789\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.300   0.394833  0.320714  0.864667\n",
      "DecisionTree      0.385   0.362500  0.355333  0.843333\n",
      "GaussianNB        0.635   0.252797  0.347299  0.650000\n",
      "KNeighbors        0.355   0.386667  0.350333  0.848000\n",
      "LogitRegression   0.270   0.326500  0.278381  0.839333\n",
      "RandomForest      0.350   0.378833  0.340762  0.850000\n",
      "rbf_SVC           0.000   0.000000  0.000000  0.866667\n",
      "\n",
      "When features selected= 10, escape=features_list\n",
      "========Top 10 features and their scores:\n",
      "total_stock_value              34.9075976019\n",
      "exercised_stock_options        33.2879034061\n",
      "bonus                          20.6883364652\n",
      "salary                         18.6050424233\n",
      "restricted_stock               16.2334432323\n",
      "to_poi_fraction                14.4941026933\n",
      "deferred_income                10.4590114322\n",
      "long_term_incentive            8.78693453741\n",
      "total_payments                 8.6034931369\n",
      "shared_receipt_with_poi        7.35815751949\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.250   0.318333  0.266667  0.862000\n",
      "DecisionTree      0.435   0.447667  0.412238  0.852667\n",
      "GaussianNB        0.395   0.445000  0.397000  0.858667\n",
      "KNeighbors        0.300   0.307167  0.283929  0.823333\n",
      "LogitRegression   0.245   0.142790  0.168826  0.684000\n",
      "RandomForest      0.270   0.286500  0.258048  0.840000\n",
      "rbf_SVC           0.000   0.000000  0.000000  0.866667\n"
     ]
    }
   ],
   "source": [
    "#With all features escape from all transfromation,\n",
    "#more features are selected, the better performance most of models give.\n",
    "#When all features are selected, the performance get boost most.\n",
    "#GaussianNB gives the highest Recall.\n",
    "\n",
    "print \"\\nWhen  features selected= 22, escape=features_list\"\n",
    "print models_comparison(22,escape=features_list)\n",
    "print \"\\nWhen features selected= 20, escape=features_list\"\n",
    "print models_comparison(20,escape=features_list)\n",
    "print \"\\nWhen features selected= 10, escape=features_list\"\n",
    "print models_comparison(10,escape=features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**GaussianNB: The most alert, sometimes over-alert. Is this good?**  \n",
    "\n",
    "With no log10 transformation and normalization performed,GaussianNB gives the highest recall score at K=22, which means no feature selection conducted. Althought this model gives recall as high as 0.82, it's bad in its precision(0.15), and gives a worst ever accuracy (0.34). Only 1 of the seven person supected of fraud are actually criminal candidate.  However with the highly biased recall power, it's  extremely useful in preventing crime for a precaution system (just like for a fire alarm, we would rather stand with false alarms instead of the risk of a missed reported accident. In contrast, rbf_svc here is excellent in accuracy--by giving all negative guess, which is safest. But as alarm its just useless.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When  features selected= 10, escape=None\n",
      "========Top 10 features and their scores:\n",
      "other                          19.6575366659\n",
      "to_poi_fraction                16.6636560959\n",
      "expenses                       14.9631978528\n",
      "bonus                          12.9509369506\n",
      "salary                         10.3048634375\n",
      "from_poi_to_this_person        8.35166246726\n",
      "from_this_person_to_poi        8.089761319\n",
      "total_stock_value              7.45637342328\n",
      "total_payments                 7.31584157833\n",
      "restricted_stock               7.16702125727\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.295   0.353167  0.303381  0.846667\n",
      "DecisionTree      0.365   0.354833  0.335048  0.838667\n",
      "GaussianNB        0.885   0.279091  0.418977  0.657333\n",
      "KNeighbors        0.375   0.352333  0.342714  0.830000\n",
      "LogitRegression   0.000   0.000000  0.000000  0.866667\n",
      "RandomForest      0.305   0.333833  0.297429  0.842667\n",
      "rbf_SVC           0.325   0.313167  0.302381  0.845333\n",
      "\n",
      "When  features selected= 5, escape=None\n",
      "========Top 5 features and their scores:\n",
      "other                          19.6575366659\n",
      "to_poi_fraction                16.6636560959\n",
      "expenses                       14.9631978528\n",
      "bonus                          12.9509369506\n",
      "salary                         10.3048634375\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.290   0.241667  0.251286  0.830000\n",
      "DecisionTree      0.285   0.256389  0.250589  0.815333\n",
      "GaussianNB        0.885   0.320099  0.463053  0.706667\n",
      "KNeighbors        0.425   0.409167  0.390333  0.853333\n",
      "LogitRegression   0.000   0.000000  0.000000  0.866667\n",
      "RandomForest      0.235   0.241500  0.220000  0.816667\n",
      "rbf_SVC           0.205   0.200000  0.192000  0.839333\n",
      "\n",
      "When  features selected= 4, escape=None (GaussianNB here gives the most satisfying performance)\n",
      "========Top 4 features and their scores:\n",
      "other                          19.6575366659\n",
      "to_poi_fraction                16.6636560959\n",
      "expenses                       14.9631978528\n",
      "bonus                          12.9509369506\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.280   0.258667  0.251810  0.837333\n",
      "DecisionTree      0.295   0.298357  0.275873  0.832667\n",
      "GaussianNB        0.865   0.361810  0.499865  0.751333\n",
      "KNeighbors        0.365   0.334167  0.327333  0.840000\n",
      "LogitRegression   0.000   0.000000  0.000000  0.866667\n",
      "RandomForest      0.285   0.311333  0.277762  0.838000\n",
      "rbf_SVC           0.135   0.140833  0.131000  0.842000\n",
      "\n",
      "When  features selected= 3, escape=None\n",
      "========Top 3 features and their scores:\n",
      "other                          19.6575366659\n",
      "to_poi_fraction                16.6636560959\n",
      "expenses                       14.9631978528\n",
      "\n",
      "                 recall  precision    fscore  accuracy\n",
      "AdaBoost          0.425   0.411690  0.390206  0.840667\n",
      "DecisionTree      0.400   0.389333  0.371571  0.848667\n",
      "GaussianNB        0.790   0.372341  0.489501  0.785333\n",
      "KNeighbors        0.395   0.366357  0.354516  0.840000\n",
      "LogitRegression   0.000   0.000000  0.000000  0.866667\n",
      "RandomForest      0.300   0.302833  0.281143  0.847333\n",
      "rbf_SVC           0.080   0.079167  0.076667  0.836667\n"
     ]
    }
   ],
   "source": [
    "#With all features log10 transformed and normalized,\n",
    "#Rebundant features pose negative effect on the performance. \n",
    "#The peak appear when number of selected features equals to 4.\n",
    "\n",
    "print \"\\nWhen  features selected= 10, escape=None\"\n",
    "print models_comparison(10)\n",
    "print \"\\nWhen  features selected= 5, escape=None\"\n",
    "print models_comparison(5)\n",
    "print \"\\nWhen  features selected= 4, escape=None (GaussianNB here gives the most satisfying performance)\"\n",
    "print models_comparison(4)\n",
    "print \"\\nWhen  features selected= 3, escape=None\"\n",
    "print models_comparison(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In final model, selected features number = 4, classifier = GaussianN, feature scaling: all log10 transformated and normalized**   GaussianNB under this condition gives a very satisfying performance, with *recall 0.87  precision 0.36 fscore 0.50 accuracy 0.75*, looks a very nice alarm! And we use only 4 features out of the total 22 here, consistant with our speculation that such small dataset need only a few features to give best performance and avoid overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_transformed_df,top_scored=preprocess(4,translist=features_list)\n",
    "\n",
    "#dump the list of features used in final_models\n",
    "top_features=[]\n",
    "for feature,score in top_scored:\n",
    "    top_features.append(feature)\n",
    "    \n",
    "with open('my_feature_list.pkl', \"w\") as featurelist_outfile:\n",
    "     pickle.dump(['poi']+top_features, featurelist_outfile)\n",
    "\n",
    "#dump the processed dataset used in final_models\n",
    "selected_transformed_dataset=defaultdict(lambda:{})\n",
    "\n",
    "for idx in selected_transformed_df.index:\n",
    "    selected_transformed_dataset[idx]['poi']=nnz_df.ix[idx,'poi']    \n",
    "    for cln in selected_transformed_df.columns:\n",
    "        selected_transformed_dataset[idx][cln]=selected_transformed_df.ix[idx,cln]\n",
    "\n",
    "with open('my_dataset.pkl','w') as f:\n",
    "    pickle.dump(dict(selected_transformed_dataset),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### 3.  What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  \n",
    "\n",
    "I ended up using GaussianNB, the most alert. Besides I have tried Kneighbors, logitregression, decision trees, adaboost, randomforest, and support vector machine as well. I found that **GaussianNB** is always the most alert and most likely gives false alarm. **Logistic Regression and SVC(kernel=rbf)** tends giving non-POI guess to secure high accuracy when it's not confident in choosing which side; adaboost,random forest,decision tree and kneighbors algorithms give consistently dependable scores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### 4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?  \n",
    "\n",
    "Tuning parameters is to choose the most suitable setting up for our model to perform best on specific dataset. If this is not done well the potential of our model won't be exploited and we end up wasting its prediction power for nothing.  I tune the parameters using automated algorithm **GridCV**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  \n",
    "\n",
    "Validation is to check the consistent performance of our model on the same probelm by testing it on a dataset different but related to our training dataset. It is a must-do to prevent overfitting, a classic mistake if validation is not done properly.  I validate it by **StratifiedShuffleSplit**. I want to take full use of all data points and avoid the random error Because my dataset is very small. StratifiedShuffleSplitsplits our data into training and test in different run and make sure every point is sampled and finally return us an averaged performance.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "**GaussianNB (Recall:0.87 Precision:0.36 Fscore:0.50 Accuracy:0.75)**  \n",
    "\n",
    "Recall: GaussianNB is good at recognizing criminals and making bold identifications. When a POI appears, it can recoginize 87% of them.   \n",
    "Precision: however it is too alert in POI detecting that it frequently gives false alarms. Only roughly 1 out of the 3 identified POI is actually POI, while the rest of them are in fact innocent.   \n",
    "Accuracy: the accuracy of GaussianNB is almost always the lowest comparing with others under same situation. But considering its brave attempts of POI detecting, the accuracy is fairly nice. 70% of its overall claims are true.   \n",
    "\n",
    "with the highly biased recalling power and a fairly good precision, this model is  very useful in preventing crime from bud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
